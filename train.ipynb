{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image and mask paths\n",
    "def load_image_paths(base_dir, scene_id):\n",
    "    #before_image1 = os.path.join(base_dir, f'{scene_id}_change-0.png')\n",
    "    before_image1 = base_dir + '/' + scene_id + '_change-0.png'\n",
    "    #after_image1 = os.path.join(base_dir, f'{scene_id}_change-1.png')\n",
    "    after_image1 = base_dir + '/' + scene_id + '_change-1.png'\n",
    "    #mask = os.path.join(base_dir, f'{scene_id}_mask.png')\n",
    "    mask = base_dir + '/' + scene_id + '_mask.png'\n",
    "    return before_image1, after_image1, mask\n",
    "\n",
    "def get_image_pairs_and_masks(base_dir, scene_ids):\n",
    "    image_pairs = []\n",
    "    masks = []\n",
    "    \n",
    "    \n",
    "    for scene_id in scene_ids:\n",
    "        before_image1, after_image1, mask = load_image_paths(base_dir, scene_id)\n",
    "        image_pairs.append((before_image1, after_image1))\n",
    "        masks.append(mask)\n",
    "        \n",
    "    return image_pairs, masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class ChangeDetectionDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_pairs = None, masks = None, batch_size=4, image_size=(256, 256), shuffle=True):\n",
    "        self.json_file = open('utils/synthetic_anno.json')\n",
    "        self.coco = json.load(self.json_file) \n",
    "        self.process_images()\n",
    "        if(image_pairs is None and masks is None):\n",
    "            self.image_pairs, self.masks = self.get_image_pairs_and_masks('data/renders_multicam_diff_1')\n",
    "        else:\n",
    "            self.image_pairs = image_pairs\n",
    "            self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.image_pairs))\n",
    "        #self.on_epoch_end()\n",
    "        \n",
    "    def get_image_pairs_and_masks(self, base_dir):\n",
    "        image_pairs = []\n",
    "        masks = []\n",
    "        \n",
    "        scene_ids = [item['scene'] for item in self.coco['images']]\n",
    "        \n",
    "        for scene_id in scene_ids:\n",
    "            before_image1, after_image1, mask = load_image_paths(base_dir, scene_id)\n",
    "            image_pairs.append((before_image1, after_image1))\n",
    "            masks.append(mask)\n",
    "            \n",
    "            \n",
    "        return image_pairs, masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_pairs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_image_pairs = [self.image_pairs[i] for i in indices]\n",
    "        batch_masks = [self.masks[i] for i in indices]\n",
    "        \n",
    "        #X1, X2, y = self.__data_generation(batch_image_pairs, batch_masks)\n",
    "        X, y = self.__data_generation(batch_image_pairs, batch_masks)\n",
    "        #return [X1, X2], y\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.image_pairs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    # def __data_generation(self, batch_image_pairs, batch_masks):\n",
    "    #     X1 = np.zeros((self.batch_size, *self.image_size, 3), dtype=np.float32)\n",
    "    #     X2 = np.zeros((self.batch_size, *self.image_size, 3), dtype=np.float32)\n",
    "    #     y = np.zeros((self.batch_size, *self.image_size, 3), dtype=np.float32)  # 3 channels for color-coded mask\n",
    "        \n",
    "    #     for i, (img_paths, mask_path) in enumerate(zip(batch_image_pairs, batch_masks)):\n",
    "    #         before_img = img_to_array(load_img(img_paths[0], target_size=self.image_size)) / 255.0\n",
    "    #         after_img = img_to_array(load_img(img_paths[1], target_size=self.image_size)) / 255.0\n",
    "    #         mask = img_to_array(load_img(mask_path, target_size=self.image_size)) / 255.0\n",
    "            \n",
    "    #         X1[i, :, :, :] = before_img\n",
    "    #         X2[i, :, :, :] = after_img\n",
    "    #         y[i, :, :, :] = mask\n",
    "        \n",
    "    #     return X1, X2, y\n",
    "    \n",
    "    def __data_generation(self, batch_image_pairs, batch_masks):\n",
    "        X = np.zeros((self.batch_size, *self.image_size, 6), dtype=np.float32)  # 6 channels for concatenated images\n",
    "        y = np.zeros((self.batch_size, *self.image_size, 4), dtype=np.float32)  # 3 channels for color-coded mask\n",
    "        \n",
    "        for i, (img_paths, mask_path) in enumerate(zip(batch_image_pairs, batch_masks)):\n",
    "            before_img = img_to_array(load_img(img_paths[0], target_size=self.image_size)) / 255.0\n",
    "            after_img = img_to_array(load_img(img_paths[1], target_size=self.image_size)) / 255.0\n",
    "            mask = img_to_array(load_img(mask_path, target_size=self.image_size)) / 255.0\n",
    "\n",
    "            mask = self.rgb_to_onehot(mask)\n",
    "            \n",
    "            X[i, :, :, :3] = before_img\n",
    "            X[i, :, :, 3:] = after_img\n",
    "            y[i, :, :, :] = mask\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def process_images(self):\n",
    "        self.images = {}\n",
    "        for image in self.coco['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in self.images:\n",
    "                print(\"ERROR: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                self.images[image_id] = image\n",
    "\n",
    "    def rgb_to_onehot(rgb_image):\n",
    "        print(f\"rgb_image shape: {rgb_image.shape}\")\n",
    "        onehot_image = np.zeros((rgb_image.shape[0], rgb_image.shape[1], 4), dtype=np.float32)\n",
    "        onehot_image[(rgb_image == [0, 0, 0]).all(axis=-1)] = [1, 0, 0, 0]     # Background\n",
    "        onehot_image[(rgb_image == [255, 0, 0]).all(axis=-1)] = [0, 1, 0, 0]   # Red (Taken)\n",
    "        onehot_image[(rgb_image == [0, 255, 0]).all(axis=-1)] = [0, 0, 1, 0]   # Green (Added)\n",
    "        onehot_image[(rgb_image == [0, 0, 255]).all(axis=-1)] = [0, 0, 0, 1]   # Blue (Shifted)\n",
    "        print(f\"onehot_image shape: {onehot_image.shape}\")\n",
    "        return onehot_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChangeDetectionDataset()\n",
    "\n",
    "print(dataset.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with provided scene_ids\n",
    "base_dir = 'data/renders_multicam_diff1'\n",
    "images_arr = dataset.images\n",
    "\n",
    "# Split dataset\n",
    "image_pairs_train, image_pairs_test, masks_train, masks_test = train_test_split(\n",
    "    dataset.image_pairs, dataset.masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "image_pairs_train, image_pairs_validation, masks_train, masks_validation = train_test_split(\n",
    "    image_pairs_train, masks_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChangeDetectionDataset(image_pairs_train, masks_train, batch_size=4)\n",
    "validation_dataset = ChangeDetectionDataset(image_pairs_validation, masks_validation, batch_size=4)\n",
    "test_dataset = ChangeDetectionDataset(image_pairs_test, masks_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.image_pairs)\n",
    "print(validation_dataset.image_pairs)\n",
    "print(test_dataset.image_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset.image_pairs))\n",
    "print(train_dataset.__len__())\n",
    "#X, y = train_dataset.__getitem__(0)\n",
    "\n",
    "# print(\"X shape:\", X.shape)\n",
    "# print(\"y shape:\", y.shape)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def siamese_unet(input_size=(256, 256, 3)):\n",
    "#     inputs1 = Input(input_size)\n",
    "#     inputs2 = Input(input_size)\n",
    "    \n",
    "#     def unet_encoder(inputs):\n",
    "#         conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "#         conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "#         pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "#         conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "#         conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "#         pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        \n",
    "#         conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "#         conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "        \n",
    "#         return conv1, conv2, conv3\n",
    "    \n",
    "#     enc1 = unet_encoder(inputs1)\n",
    "#     enc2 = unet_encoder(inputs2)\n",
    "    \n",
    "#     def unet_decoder(enc1, enc2):\n",
    "#         merge = concatenate([enc1[2], enc2[2]])\n",
    "        \n",
    "#         conv4 = Conv2D(512, 3, activation='relu', padding='same')(merge)\n",
    "#         conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "        \n",
    "#         up5 = concatenate([Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4), enc1[1], enc2[1]])\n",
    "#         conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
    "#         conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "        \n",
    "#         up6 = concatenate([Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5), enc1[0], enc2[0]])\n",
    "#         conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
    "#         conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "        \n",
    "#         conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv6)\n",
    "#         conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "        \n",
    "#         outputs = Conv2D(3, 1, activation='softmax')(conv7)  # 3 channels for color-coded output\n",
    "        \n",
    "#         return outputs\n",
    "    \n",
    "#     outputs = unet_decoder(enc1, enc2)\n",
    "#     model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# model = siamese_unet()\n",
    "\n",
    "# initial_learning_rate = 1e-4 \n",
    "# optimizer = Adam(learning_rate=initial_learning_rate)\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()\n",
    "\n",
    "    \n",
    "def unet_model(input_size=(256, 256, 6)):  # 6 channels for concatenated images\n",
    "    inputs = Input(input_size)\n",
    "    \n",
    "    def unet_encoder(inputs):\n",
    "        conv1 = Conv2D(16, 6, activation='relu', padding='same', kernel_regularizer=l2(0.001))(inputs)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        conv1 = Conv2D(16, 6, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv1)\n",
    "        conv1 = BatchNormalization()(conv1)\n",
    "        pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        pool1 = Dropout(0.2)(pool1)\n",
    "        \n",
    "        conv2 = Conv2D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(pool1)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        conv2 = Conv2D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv2)\n",
    "        conv2 = BatchNormalization()(conv2)\n",
    "        pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        pool2 = Dropout(0.2)(pool2)\n",
    "        \n",
    "        conv3 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(pool2)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        conv3 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv3)\n",
    "        conv3 = BatchNormalization()(conv3)\n",
    "        \n",
    "        return conv1, conv2, conv3\n",
    "    \n",
    "    enc = unet_encoder(inputs)\n",
    "    \n",
    "    def unet_decoder(enc):\n",
    "        conv1, conv2, conv3 = enc\n",
    "        \n",
    "        conv4 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv3)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        conv4 = Conv2D(128, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv4)\n",
    "        conv4 = BatchNormalization()(conv4)\n",
    "        \n",
    "        up5 = concatenate([Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv4), conv2])\n",
    "        conv5 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(up5)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        conv5 = Conv2D(64, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv5)\n",
    "        conv5 = BatchNormalization()(conv5)\n",
    "        \n",
    "        up6 = concatenate([Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(conv5), conv1])\n",
    "        conv6 = Conv2D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(up6)\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "        conv6 = Conv2D(32, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv6)\n",
    "        conv6 = BatchNormalization()(conv6)\n",
    "        \n",
    "        conv7 = Conv2D(16, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv6)\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "        conv7 = Conv2D(16, 3, activation='relu', padding='same', kernel_regularizer=l2(0.001))(conv7)\n",
    "        conv7 = BatchNormalization()(conv7)\n",
    "        \n",
    "        outputs = Conv2D(4, 1, activation='softmax')(conv7)  # 3 channels for color-coded output\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    outputs = unet_decoder(enc)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = unet_model()\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_dataset)\n",
    "plt.imshow(predictions[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_to_rgb(onehot_mask):\n",
    "    rgb_image = np.zeros((onehot_mask.shape[0], onehot_mask.shape[1], 3), dtype=np.float32)\n",
    "    rgb_image[onehot_mask[:, :, 0] == 1] = [0, 0, 0]   # Background\n",
    "    rgb_image[onehot_mask[:, :, 1] == 1] = [255, 0, 0] # Red (Taken)\n",
    "    rgb_image[onehot_mask[:, :, 2] == 1] = [0, 255, 0] # Green (Added)\n",
    "    rgb_image[onehot_mask[:, :, 3] == 1] = [0, 0, 255] # Blue (Shifted)\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_predictions(model, dataset, batch_index=0):\n",
    "    # Get a batch of data\n",
    "    X, y_true = dataset[batch_index]\n",
    "    \n",
    "    summed_mask = np.sum(y_true, axis=3)\n",
    "    cv2.imshow(\"Summed Mask\", summed_mask)\n",
    "    print('SHAPES: ', X.shape, y_true.shape)\n",
    "    X1, X2 = np.split(X, 2, axis=-1)  # Split concatenated images back into two images\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Convert predictions and ground truth to class labels\n",
    "    y_true_labels = np.argmax(y_true, axis=-1, keepdims=True)\n",
    "    print('Y_TRUE_LABELS: ', y_true_labels.shape)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=-1, keepdims=True)\n",
    "    print('Y_PRED_LABELS: ', y_pred_labels.shape)\n",
    "\n",
    "    y_true_rgb = np.array([onehot_to_rgb(np.eye(4)[y_true_labels[i].squeeze()]) for i in range(y_true_labels.shape[0])])\n",
    "    print('Y_TRUE_RGB: ', y_true_rgb.shape)\n",
    "    y_pred_rgb = np.array([onehot_to_rgb(np.eye(4)[y_pred_labels[i].squeeze()]) for i in range(y_pred_labels.shape[0])])\n",
    "\n",
    "    #y_true_rgb = np.array([onehot_to_rgb(y) for y in y_true_labels])\n",
    "    \n",
    "    #y_pred_rgb = np.array([onehot_to_rgb(y) for y in y_pred_labels])\n",
    "\n",
    "    # Function to plot images and masks\n",
    "    def plot_comparison(before_img, after_img, true_mask, pred_mask, index=0):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "        axs[0].imshow(before_img[index])\n",
    "        axs[0].set_title('Before Image')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(after_img[index])\n",
    "        axs[1].set_title('After Image')\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].imshow(true_mask[index], cmap='jet', alpha=0.5)\n",
    "        axs[2].set_title('Ground Truth Mask')\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        axs[3].imshow(pred_mask[index], cmap='jet', alpha=0.5)\n",
    "        axs[3].set_title('Predicted Mask')\n",
    "        axs[3].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the results for the first image in the batch\n",
    "    plot_comparison(X1, X2, y_true_rgb, y_pred_rgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "visualize_predictions(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def unet_model(input_size=(256, 256, 3)):\n",
    "#     inputs1 = Input(input_size)\n",
    "#     inputs2 = Input(input_size)\n",
    "    \n",
    "#     def unet_encoder(inputs):\n",
    "#         conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "#         conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
    "#         pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "        \n",
    "#         conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
    "#         conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
    "#         pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "        \n",
    "#         conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
    "#         conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
    "        \n",
    "#         return conv1, conv2, conv3\n",
    "    \n",
    "#     enc1 = unet_encoder(inputs1)\n",
    "#     enc2 = unet_encoder(inputs2)\n",
    "    \n",
    "#     def unet_decoder(enc1, enc2):\n",
    "#         merge = concatenate([enc1[2], enc2[2]])\n",
    "        \n",
    "#         conv4 = Conv2D(512, 3, activation='relu', padding='same')(merge)\n",
    "#         conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
    "        \n",
    "#         up5 = concatenate([Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv4), enc1[1], enc2[1]])\n",
    "#         conv5 = Conv2D(256, 3, activation='relu', padding='same')(up5)\n",
    "#         conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
    "        \n",
    "#         up6 = concatenate([Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv5), enc1[0], enc2[0]])\n",
    "#         conv6 = Conv2D(128, 3, activation='relu', padding='same')(up6)\n",
    "#         conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
    "        \n",
    "#         conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv6)\n",
    "#         conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
    "        \n",
    "#         outputs = Conv2D(3, 1, activation='softmax')(conv7)  # 3 channels for color-coded output\n",
    "        \n",
    "#         return outputs\n",
    "    \n",
    "#     outputs = unet_decoder(enc1, enc2)\n",
    "#     model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "#     return model\n",
    "\n",
    "# model = unet_model()\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.fit(train_dataset, validation_data=validation_dataset, epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
