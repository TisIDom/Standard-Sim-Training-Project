{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, concatenate, Conv2DTranspose, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.preprocessing.image as prep\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import segmentation_models as sm\n",
    "\n",
    "import OpenEXR as exr\n",
    "import Imath\n",
    "\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_paths(base_dir, scene_id):\n",
    "    normal_before = base_dir + '/' + scene_id + '_change-0.png'\n",
    "    normal_after = base_dir + '/' + scene_id + '_change-1.png'\n",
    "    nomats_before = base_dir + '/' + scene_id + '_change-0-nomats0001.png'\n",
    "    nomats_after = base_dir + '/' + scene_id + '_change-1-nomats0001.png'\n",
    "    randommats_before = base_dir + '/' + scene_id + '_change-0-randommats.png'\n",
    "    randommats_after = base_dir + '/' + scene_id + '_change-1-randommats.png'\n",
    "    depth_before = base_dir + '/' + scene_id + '_change-0-depth0001.exr'\n",
    "    depth_after = base_dir + '/' + scene_id + '_change-1-depth0001.exr'\n",
    "    mask = base_dir + '/' + scene_id + '_mask.png'\n",
    "    return normal_before, normal_after, nomats_before, nomats_after, randommats_before, randommats_after, depth_before, depth_after, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generator\n",
    "class ChangeDetectionDataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_pairs = None, depth_pairs = None, masks = None, batch_size=1, image_size=(512, 512), shuffle=True, augment=True):\n",
    "        self.json_file = open('utils/synthetic_anno.json')\n",
    "        self.coco = json.load(self.json_file) \n",
    "        self.process_images()\n",
    "        if(image_pairs is None and masks is None):\n",
    "            self.image_pairs, self.depth_pairs, self.masks = self.get_image_pairs_and_masks('data/renders_multicam_diff_1')\n",
    "        else:\n",
    "            self.image_pairs = image_pairs\n",
    "            self.depth_pairs = depth_pairs\n",
    "            self.masks = masks\n",
    "        self.batch_size = batch_size\n",
    "        self.image_size = image_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.image_pairs))\n",
    "        self.augment = augment\n",
    "        if self.augment:\n",
    "            self.image_datagen = prep.ImageDataGenerator(\n",
    "                rotation_range=180,\n",
    "                width_shift_range=0.2,\n",
    "                height_shift_range=0.2,\n",
    "                #shear_range=0.2,\n",
    "                zoom_range=0.2,\n",
    "                horizontal_flip=True,\n",
    "                fill_mode='nearest'\n",
    "            )\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def get_image_pairs_and_masks(self, base_dir):\n",
    "        masks = []\n",
    "        pairs = []\n",
    "        depth_pairs = []\n",
    "        \n",
    "        scene_ids = [item['scene'] for item in self.coco['images']]\n",
    "\n",
    "        for scene_id in scene_ids:\n",
    "            normal_before, normal_after, nomats_before, nomats_after, randommats_before, randommats_after, depth_before, depth_after, mask = load_image_paths(base_dir, scene_id)\n",
    "            if not os.path.exists(normal_before):\n",
    "                continue\n",
    "            if not os.path.exists(normal_after):\n",
    "                continue\n",
    "            if not os.path.exists(nomats_before):\n",
    "                continue\n",
    "            if not os.path.exists(nomats_after):\n",
    "                continue\n",
    "            if not os.path.exists(randommats_before):\n",
    "                continue\n",
    "            if not os.path.exists(randommats_after):\n",
    "                continue\n",
    "            if not os.path.exists(depth_before):\n",
    "                continue\n",
    "            if not os.path.exists(depth_after):\n",
    "                continue\n",
    "            if not os.path.exists(mask):\n",
    "                continue\n",
    "            pairs.append((normal_before, normal_after))\n",
    "            pairs.append((nomats_before, nomats_after))\n",
    "            pairs.append((randommats_before, randommats_after))\n",
    "            \n",
    "            depth_pairs.append((depth_before, depth_after))\n",
    "            depth_pairs.append((depth_before, depth_after))\n",
    "            depth_pairs.append((depth_before, depth_after))\n",
    "            \n",
    "            masks.append(mask)\n",
    "            masks.append(mask)\n",
    "            masks.append(mask)\n",
    "\n",
    "        return pairs, depth_pairs, masks\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_pairs) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_image_pairs = [self.image_pairs[i] for i in indices]\n",
    "        batch_depth_pairs = [self.depth_pairs[i] for i in indices]\n",
    "        batch_masks = [self.masks[i] for i in indices]\n",
    "        \n",
    "        X, y = self.__data_generation(batch_image_pairs, batch_depth_pairs, batch_masks)\n",
    "        \n",
    "        return tf.convert_to_tensor(X), tf.convert_to_tensor(y)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.image_pairs))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __data_generation(self, batch_image_pairs, batch_depth_pairs, batch_masks):\n",
    "        X = np.zeros((self.batch_size, *self.image_size, 8), dtype=np.float32)  # 6 channels for concatenated images\n",
    "        y = np.zeros((self.batch_size, *self.image_size, 4), dtype=np.float32)  # 3 channels for color-coded mask\n",
    "        \n",
    "        for i, (img_paths, depth_paths, mask_path) in enumerate(zip(batch_image_pairs, batch_depth_pairs, batch_masks)):\n",
    "            before_img = img_to_array(load_img(img_paths[0], target_size=self.image_size)) / 255.0\n",
    "            after_img = img_to_array(load_img(img_paths[1], target_size=self.image_size)) / 255.0\n",
    "            before_depth = self.load_depth_image(depth_paths[0], self.image_size) \n",
    "            after_depth = self.load_depth_image(depth_paths[1], self.image_size)\n",
    "            mask = img_to_array(load_img(mask_path, target_size=self.image_size))\n",
    "            \n",
    "            if self.augment:\n",
    "                seed = np.random.randint(1e6)\n",
    "                before_img = self.image_datagen.random_transform(before_img, seed=seed)\n",
    "                after_img = self.image_datagen.random_transform(after_img, seed=seed)\n",
    "                before_depth = self.image_datagen.random_transform(np.expand_dims(before_depth, axis=-1), seed=seed)\n",
    "                after_depth = self.image_datagen.random_transform(np.expand_dims(after_depth, axis=-1), seed=seed)\n",
    "                mask = self.image_datagen.random_transform(mask, seed=seed)\n",
    "\n",
    "                before_depth = np.squeeze(before_depth)\n",
    "                after_depth = np.squeeze(after_depth)\n",
    "            \n",
    "\n",
    "            mask = self.rgb_to_onehot(mask)\n",
    "            \n",
    "            X[i, :, :, :3] = before_img\n",
    "            X[i, :, :, 3:6] = after_img\n",
    "            X[i, :, :, 6] = before_depth\n",
    "            X[i, :, :, 7] = after_depth\n",
    "            y[i, :, :, :] = mask\n",
    "            \n",
    "        return X, y\n",
    "    \n",
    "    def process_images(self):\n",
    "        self.images = {}\n",
    "        for image in self.coco['images']:\n",
    "            image_id = image['id']\n",
    "            if image_id in self.images:\n",
    "                print(\"ERROR: Skipping duplicate image id: {}\".format(image))\n",
    "            else:\n",
    "                self.images[image_id] = image\n",
    "\n",
    "    def rgb_to_onehot(self,rgb_image):\n",
    "        onehot_image = np.zeros((rgb_image.shape[0], rgb_image.shape[1], 4), dtype=np.float32)\n",
    "        onehot_image[(rgb_image == [0, 0, 0]).all(axis=-1)] = [1, 0, 0, 0]     # Background\n",
    "        onehot_image[(rgb_image == [255, 0, 0]).all(axis=-1)] = [0, 1, 0, 0]   # Red (Taken)\n",
    "        onehot_image[(rgb_image == [0, 255, 0]).all(axis=-1)] = [0, 0, 1, 0]   # Green (Added)\n",
    "        onehot_image[(rgb_image == [0, 0, 255]).all(axis=-1)] = [0, 0, 0, 1]   # Blue (Shifted)\n",
    "        return onehot_image\n",
    "    \n",
    "    def onehot_to_rgb(self, onehot_mask):\n",
    "        rgb_image = np.zeros((onehot_mask.shape[0], onehot_mask.shape[1], 3), dtype=np.float32)\n",
    "        rgb_image[onehot_mask[:, :, 0] == 1] = [0, 0, 0]   # Background\n",
    "        rgb_image[onehot_mask[:, :, 1] == 1] = [255, 0, 0] # Red (Taken)\n",
    "        rgb_image[onehot_mask[:, :, 2] == 1] = [0, 255, 0] # Green (Added)\n",
    "        rgb_image[onehot_mask[:, :, 3] == 1] = [0, 0, 255] # Blue (Shifted)\n",
    "        return rgb_image\n",
    "    \n",
    "    def load_depth_image(self, path, target_size):\n",
    "        file = exr.InputFile(path)\n",
    "        dw = file.header()['dataWindow']\n",
    "        size = (dw.max.x - dw.min.x + 1, dw.max.y - dw.min.y + 1)\n",
    "\n",
    "        FLOAT = Imath.PixelType(Imath.PixelType.FLOAT)\n",
    "        (R, G, B) = file.channels(\"RGB\", FLOAT)\n",
    "\n",
    "        depth_map = np.fromstring(R, dtype=np.float32)\n",
    "        depth_map = np.reshape(depth_map, (size[1], size[0]))\n",
    "        \n",
    "        depth_map_resized = cv2.resize(depth_map, target_size, interpolation=cv2.INTER_LINEAR)\n",
    "        return depth_map_resized\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChangeDetectionDataset(augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with provided scene_ids\n",
    "base_dir = 'data/renders_multicam_diff1'\n",
    "images_arr = dataset.images\n",
    "\n",
    "# Split dataset\n",
    "image_pairs_train, image_pairs_test, depth_pairs_train, depth_pairs_test,  masks_train, masks_test = train_test_split(\n",
    "    dataset.image_pairs, dataset.depth_pairs, dataset.masks, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "image_pairs_train, image_pairs_validation, depth_pairs_train, depth_pairs_validation, masks_train, masks_validation = train_test_split(\n",
    "    image_pairs_train, depth_pairs_train, masks_train, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ChangeDetectionDataset(image_pairs_train, depth_pairs_train, masks_train)\n",
    "validation_dataset = ChangeDetectionDataset(image_pairs_validation, depth_pairs_validation, masks_validation)\n",
    "test_dataset = ChangeDetectionDataset(image_pairs_test, depth_pairs_test, masks_test, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masks = []\n",
    "\n",
    "for i in range(0, dataset.__len__()):\n",
    "    mask = dataset.masks[i]\n",
    "    mask = dataset.rgb_to_onehot(img_to_array(load_img(mask, target_size=(256, 256))))\n",
    "    masks.append(mask)\n",
    "\n",
    "y_train_flat = np.argmax(masks, axis=-1).flatten()\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train_flat), y=y_train_flat)\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "\n",
    "print(\"Class weights:\", class_weights_dict)\n",
    "\n",
    "class_weights_tensor = tf.constant(class_weights, dtype=tf.float32)\n",
    "\n",
    "def weighted_categorical_crossentropy(y_true, y_pred):\n",
    "    # Compute the categorical crossentropy loss\n",
    "    loss = CategoricalCrossentropy()(y_true, y_pred)\n",
    "\n",
    "    weights = tf.reduce_sum(class_weights_tensor * y_true, axis=-1)\n",
    "    weighted_loss = loss * weights\n",
    "    \n",
    "    return tf.reduce_mean(weighted_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.Unet('resnet34', input_shape=(512, 512, 8), classes=4, activation='softmax', encoder_weights=None)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=weighted_categorical_crossentropy,\n",
    "    metrics=[sm.metrics.iou_score, sm.metrics.f1_score, sm.metrics.precision, sm.metrics.recall, 'accuracy'],\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.2,          # Factor by which the learning rate will be reduced\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.00001         # Lower bound on the learning rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, batch_size=1, epochs=25, validation_data=validation_dataset, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_predictions(model, dataset, batch_index=0):\n",
    "    # Get a batch of data\n",
    "    X, y_true = dataset[batch_index]\n",
    "    \n",
    "    # Summed mask for visualization check\n",
    "    summed_mask = np.sum(y_true, axis=3)\n",
    "    print('Summed Mask Shape: ', summed_mask.shape)\n",
    "\n",
    "    # Assuming X is concatenated as [before_img, after_img, before_depth, after_depth]\n",
    "    # with 6 channels for images and 2 channels for depth\n",
    "    img_channels = 3  # number of channels in the image (RGB)\n",
    "    depth_channels = 1  # number of channels in depth (assuming 1 here, adjust if different)\n",
    "\n",
    "    # Split the data back into before images, after images, and depth images\n",
    "    X1 = X[:, :, :, :img_channels]\n",
    "    X2 = X[:, :, :, img_channels:2*img_channels]\n",
    "    depth_before = X[:, :, :, 2*img_channels:2*img_channels+depth_channels]\n",
    "    depth_after = X[:, :, :, 2*img_channels+depth_channels:]\n",
    "\n",
    "    # Predict the masks\n",
    "    y_pred = model.predict(X)\n",
    "\n",
    "    # Convert predictions and ground truth to class labels\n",
    "    y_true_labels = np.argmax(y_true, axis=-1, keepdims=True)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=-1, keepdims=True)\n",
    "\n",
    "    # Convert one-hot encoded masks to RGB for visualization\n",
    "    y_true_rgb = np.array([dataset.onehot_to_rgb(np.eye(4)[y_true_labels[i].squeeze()]) for i in range(y_true_labels.shape[0])])\n",
    "    y_pred_rgb = np.array([dataset.onehot_to_rgb(np.eye(4)[y_pred_labels[i].squeeze()]) for i in range(y_pred_labels.shape[0])])\n",
    "\n",
    "    # Function to plot images and masks\n",
    "    def plot_comparison(before_img, after_img, depth_before, depth_after, true_mask, pred_mask, index=0):\n",
    "        fig, axs = plt.subplots(1, 6, figsize=(25, 5))\n",
    "\n",
    "        axs[0].imshow(before_img[index])\n",
    "        axs[0].set_title('Before Image')\n",
    "        axs[0].axis('off')\n",
    "\n",
    "        axs[1].imshow(after_img[index])\n",
    "        axs[1].set_title('After Image')\n",
    "        axs[1].axis('off')\n",
    "\n",
    "        axs[2].imshow(depth_before[index], cmap='gray')\n",
    "        axs[2].set_title('Before Depth')\n",
    "        axs[2].axis('off')\n",
    "\n",
    "        axs[3].imshow(depth_after[index], cmap='gray')\n",
    "        axs[3].set_title('After Depth')\n",
    "        axs[3].axis('off')\n",
    "\n",
    "        axs[4].imshow(true_mask[index])\n",
    "        axs[4].set_title('Ground Truth Mask')\n",
    "        axs[4].axis('off')\n",
    "\n",
    "        axs[5].imshow(pred_mask[index])\n",
    "        axs[5].set_title('Predicted Mask')\n",
    "        axs[5].axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # Plot the results for the first image in the batch\n",
    "    plot_comparison(X1, X2, depth_before, depth_after, y_true_rgb, y_pred_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    visualize_predictions(model, test_dataset, batch_index=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siamese U-Net Implementation based on \n",
    "# @misc{růžička2020deep,\n",
    "#       title={Deep Active Learning in Remote Sensing for data efficient Change Detection}, \n",
    "#       author={Vít Růžička and Stefano D'Aronco and Jan Dirk Wegner and Konrad Schindler},\n",
    "#       year={2020},\n",
    "#       eprint={2008.11201},\n",
    "#       archivePrefix={arXiv},\n",
    "#       primaryClass={cs.CV}\n",
    "# }\n",
    "\n",
    "def unet_encoder(input_tensor, name_prefix):\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv1_1')(input_tensor)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv1_2')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D((2, 2), name=f'{name_prefix}_pool1')(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv2_1')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv2_2')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D((2, 2), name=f'{name_prefix}_pool2')(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv3_1')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv3_2')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D((2, 2), name=f'{name_prefix}_pool3')(conv3)\n",
    "\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv4_1')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv4_2')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    pool4 = MaxPooling2D((2, 2), name=f'{name_prefix}_pool4')(conv4)\n",
    "\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv5_1')(pool4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same', name=f'{name_prefix}_conv5_2')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "\n",
    "    return conv1, conv2, conv3, conv4, conv5\n",
    "\n",
    "def unet_decoder(conv1_b, conv1_a, conv2_b, conv2_a, conv3_b, conv3_a, conv4_b, conv4_a, center_b, center_a, num_classes):\n",
    "    merge1 = concatenate([center_b, center_a], axis=-1)\n",
    "    up1 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same', activation='relu')(merge1)\n",
    "    merge1 = concatenate([up1, conv4_b, conv4_a], axis=-1)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(merge1)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "\n",
    "    up2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same', activation='relu')(conv6)\n",
    "    merge2 = concatenate([up2, conv3_b, conv3_a], axis=-1)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge2)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "\n",
    "    up3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same', activation='relu')(conv7)\n",
    "    merge3 = concatenate([up3, conv2_b, conv2_a], axis=-1)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge3)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "\n",
    "    up4 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same', activation='relu')(conv8)\n",
    "    merge4 = concatenate([up4, conv1_b, conv1_a], axis=-1)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(merge4)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "\n",
    "    output = Conv2D(num_classes, (1, 1), activation='sigmoid')(conv9)\n",
    "\n",
    "    return output\n",
    "\n",
    "def siamese_unet(input_size=(512, 512, 8), num_classes=4):\n",
    "    inputs = Input(shape=input_size)\n",
    "\n",
    "    # Split the input into before and after images\n",
    "    before_image = tf.slice(inputs, [0, 0, 0, 0], [-1, -1, -1, 3])\n",
    "    after_image = tf.slice(inputs, [0, 0, 0, 3], [-1, -1, -1, 3])\n",
    "\n",
    "    # Encoder for before image\n",
    "    conv1_b, conv2_b, conv3_b, conv4_b, center_b = unet_encoder(before_image, 'before')\n",
    "    \n",
    "    # Encoder for after image\n",
    "    conv1_a, conv2_a, conv3_a, conv4_a, center_a = unet_encoder(after_image, 'after')\n",
    "\n",
    "    # Decoder\n",
    "    output = unet_decoder(conv1_b, conv1_a, conv2_b, conv2_a, conv3_b, conv3_a, conv4_b, conv4_a, center_b, center_a, num_classes)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=5,         # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    factor=0.2,          # Factor by which the learning rate will be reduced\n",
    "    patience=3,          # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=0.00001         # Lower bound on the learning rate\n",
    ")\n",
    "\n",
    "\n",
    "# EXAMPLE USAGE\n",
    "#model = siamese_unet(input_size=(512, 512, 8), num_classes=4)\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "#model.compile(optimizer=optimizer, loss=weighted_categorical_crossentropy, metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
